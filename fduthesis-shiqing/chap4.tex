

\chapter{基于辅助损失函数联合训练的改进策略}
\section{引言}
神经网络的不确定性是对其预测结果置信度的度量，不确定性分析可以帮助模型提供更可靠的决策依据。本文研究了基于高维特征概率密度估计的模型不确定性建模方法，该方法无需训练多个模型或多次推理，从而提高了不确定性的建模效率。为了进一步提升模型不确定性建模效果，第三章提出了一种基于输入扰动的策略。该策略通过引入与梯度响应相关的扰动，显著增大了分布内样本和分布外样本在概率分布空间中的差异，从而增强了模型对不确定性量化的区分能力。值得注意的是，该方法无需修改已训练的神经网络架构，能够直接应用于现有模型。

尽管基于输入扰动的策略取得了一定成效，但在特征空间优化方面本文的不确定性建模方法仍有改进空间。由于未改变训练过程，类内样本的紧密性和类间样本的可分性仍有待提升。基于高维特征概率密度估计的模型不确定性建模受特征空间结构的影响，如果能够在训练过程中优化特征空间结构，使得类内样本更紧密、类间样本更可分，则可以显著提高不确定性建模的精度和鲁棒性。

因此，本章将探讨如何通过改变训练方式，进一步优化高维特征空间结构。在深度学习应用中，有效对高维特征进行概率密度建模是一个核心挑战。通过建模这些高维特征的概率分布，不仅可以有效捕捉数据分布特征，还能解决异常检测、不确定性量化和生成建模等问题。传统的分类方法中，交叉熵损失函数（Cross-Entropy Loss）广泛应用于监督学习任务，旨在最小化预测类别与真实类别之间的差异。然而，在高维特征空间中，类别间的重叠和样本分布的稀疏性使得直接进行概率密度建模变得复杂。度量学习作为优化特征空间表示的有效方法，提供了解决这一问题的可能性。

度量学习\cite{kaya2019deep}\cite{kulis2013metric}\cite{yang2006distance}是机器学习中的一个重要分支，旨在学习适用于特定任务的距离度量。该方法通常用于度量特征空间中样本之间的相似度或差异度，广泛应用于人脸识别、图像检索、推荐系统和自然语言处理等领域。度量学习的核心目标是学习一个距离函数，使得相似样本在特征空间中接近，不相似样本远离。与传统的分类或回归任务不同，度量学习通过优化样本之间的相对距离关系，而非直接优化类别预测，从而间接提高任务性能。度量学习中的距离函数通常包括欧几里得距离、马氏距离和余弦相似度等。根据学习方式的不同，度量学习方法可以分为基于距离函数的学习算法和基于相似度/差异度的学习算法。常见的度量学习算法包括Siamese网络\cite{bromley1994signature}、三元组网络\cite{schroff2015facenet}、对比损失\cite{hadsell2006dimensionality}等。

本文提出了一种通过神经网络训练引入度量学习约束的方法，使得高维特征空间的样本分布更加适合高斯判别分析（GDA）模型进行概率密度建模。特征空间的性质对不确定性建模效果至关重要，度量学习通过优化特征表示，使相似样本更加紧密、不相似样本更远离，从而有效减少高维空间中不确定性建模的复杂性。此外，度量学习有助于提高特征分布的聚集性和可分性，从而提高概率密度估计的精度。本章设计了一种新的辅助损失函数（Auxiliary Loss），与交叉熵损失函数共同训练（如图\ref{tag:模型结构图2}所示），实现了特征空间的类内紧密性和类间可分性，从而提升了基于高维特征的概率密度建模神经网络不确定性的效果，并在OOD检测等任务上进行了验证。

此外，随着深度学习的广泛应用，特征降维技术已成为高效数据处理和分析的重要工具。由于神经网络提取的高维特征可能维度过高（如ResNet50提取的特征维度为2048维），这可能导致维度灾难\cite{murphy2012machine}、计算复杂度过高以及过拟合等问题。为了有效进行概率密度建模和计算，本章研究了降维技术的应用，提出采用主成分分析（PCA，Principal Component Analysis）作为降维方法，旨在减少GDA对高维特征建模时的存储空间需求。PCA是一种广泛应用于数据压缩、特征选择和数据可视化等领域的降维技术。本章将PCA用于高维特征降维，并在降维后的特征上重新建立GDA模型。通过理论分析PCA降维前后的时间复杂度和空间复杂度，并通过实验验证，GDA结合PCA降维能够显著减少存储空间需求并提高建模效率。
\section{主要方法}
\subsection{辅助损失函数联合训练}

首先介绍下高维特征空间上的类内紧密性和类间可分性，如图\ref{interintra}。

\begin{figure}[h]
    \centering
    \includegraphics[width=1.\linewidth]{assets/inter-intra.png}
    \caption{类内紧密性和类间可分性}
    \label{interintra}
\end{figure}

\textbf{类内紧密性}（Intra-class Compactness）指的是同一类别的样本在特征空间中分布得多么紧密，提升类内紧密性的目的是使同一类别的样本在特征空间中更接近。高类内紧密性意味着表示同一类别的样本在特征空间中聚集在一起，样本的特征能够很好地描述同一类别的特性，减少了同一类别样本之间的差异。 \textbf{类间可分性}（Inter-class Separability）指的是不同类别的样本在特征空间中的分离程度，提升类间可分性的目的是使不同类别的样本在特征空间中相距更远。高类间可分性意味着不同类别在特征空间中有更明显的分隔，类别之间有明显的边界，这有助于分类器在决策边界上更好地区分不同类别。理想状态下，在特征空间中，同一类别的样本应该分布紧密（即高类内紧密性），而不同类别的样本之间应该相距较远（即高类间可分性）。但是当数据维度增加或样本类别分布复杂时，这两者可能会受到限制。例如，噪声数据会降低类内紧密性，而类别间的相似性会降低类间可分性。通过优化类内紧密性和类间可分性，分类模型能够更高效地学习特征表示，提高分类的准确性。



CenterLoss\cite{wen2016discriminative}是一种用于深度学习中的损失函数，主要应用于人脸识别和特征嵌入学习等任务。其核心目标是通过最小化同一类别样本特征向量与该类中心之间的欧几里得距离，优化特征空间中的类内距离，从而提高模型的区分能力CenterLoss定义为：

\[
L_{\text{center}} = \frac{1}{2N} \sum_{i=1}^{N} \| f(x_i) - c_{y_i} \|^2
\]

其中，\( f(x_i) \) 表示输入样本 \( x_i \) 的特征表示，\( c_{y_i} \) 为类别 \( y_i \) 的类别中心，\(\|\cdot\|\) 是欧几里得距离，\( N \) 为样本数量。通过这种方式，CenterLoss 使得同类别的样本在特征空间中更加集中，从而提升分类性能和类别区分能力。在实际应用中，CenterLoss 通常与 Softmax 损失函数结合使用，进一步增强模型的分类效果通，过联合优化这两个损失，网络不仅能够学习到更精确的分类决策，还能够使得特征空间中的样本更加紧密。。

从CenterLoss的定义上看，仅仅包含类内紧密性的损失函数，为了提高特征空间上的类间可分性，可以考虑在其原始公式的分母上引入一些提高类间可分性的项。这种改进可以通过设计一个新的权重项，使得在计算类间距离时，能够更强调类别间的分隔性，从而增强不同类别特征的区分度。为了提高类间可分性，本文提出了一个新的辅助损失函数，在其中加入与类别间距离相关的项，从而惩罚相近类别的样本特征，使得不同类别在特征空间上更加远离。将分母中加入一个与类别中心间距相关的因子来增强类间可分性，新的损失函数公式具体如下：

\[
\text{AuxLoss} = \frac{1}{2} \sum_{i=1}^m \frac{\|x_i - C_{y_i}\|_2^2}{\sum_{j=1, j \neq y_i}^{m} \|C_{y_i} - C_j\|_2^2 + \delta}.
\]，

这里 \( C \) 是类别数，\( c_k \) 是类别 \( k \) 的中心，\( \delta \) 是一个小常数，用于数值稳定性，防止除零错误。

通过在分母中引入$\sum_{j=1, j \neq y_i}^{m} \|C_{y_i} - C_j\|_2^2$，让距离较远的类别中心在优化过程中贡献更少的惩罚项，反之，距离较近的类别中心将会对损失值产生更大的影响。这种方式鼓励类别间的分隔，使得样本的特征在同一类别内紧密、而在不同类别间具有更大的间隔。随着训练的进行，样本在特征空间中的分布会越来越偏向于形成更加清晰的类别分隔。通过调整类别间距的方式，可以有效减少类间重叠，从而增强模型对不同类别的区分能力。

这种新提出的AuxLoss可以作为辅助损失加入到交叉熵损失函数中（如图\ref{tag:模型结构图2}，进一步强化特征的聚合与类间分隔。在训练过程中，原本的交叉熵损失负责指导分类准确性，而改进后的AuxLoss则帮助网络在优化特征表示时，通过引入类别中心之间的间距来进一步优化类别分离的效果，更好地考虑到类别之间的相对距离。


\begin{figure}[h]
    \centering
    \includegraphics[width=1.\linewidth]{assets/structure2.png}
    \caption{基于高维特征概率密度建模的模型不确定性}
    \label{tag:模型结构图2}
\end{figure}


CrossEntropyLoss和AuxLoss联合训练的公式如下:
\begin{equation}
    L = L_s+\lambda * L_x
\end{equation}

其中,$L_s$是CrossEntropyLoss,公式如下:

\begin{equation}
    L_s= - \sum_{i=1}^C y_i \log\left(\frac{\exp(z_i)}{\sum_{j=1}^C \exp(z_j)}\right)
\end{equation}

$L_x$是修正后的AuxLoss，公式如下:
\begin{equation}
    L_x = \frac{1}{2}\sum_{i=1}^{m} \frac{\Vert x_i-C_{y_i}\Vert_2^2}{\sum_{j=1,j\neq y_i}^{m}\Vert C_i-C_j\Vert_2^2+\delta}
\end{equation}

上述AuxLoss,分子项通过特征向量与其对应类别中心距离的最小化保证类内紧密性，分母项通过不同类别中心的距离最大化来保证类间可分性。在具体实现时，类别中心是基于正态分布随机初始化的，它会作为模型的一部分进行学习和优化。类别中心会随着模型的训练过程而更新，类别中心作为模型参数的一部分，损失函数反向传播时，PyTorch框架通过优化器自动更新类别中心的值。关于上述AuxLoss的损失函数对参数类中心的求导，有以下的结论。

\textbf{性质4.1：}
给定的损失函数如下：

\[
L_x = \frac{1}{2} \sum_{i=1}^m \frac{\|x_i - C_{y_i}\|_2^2}{\sum_{j=1, j \neq y_i}^{m} \|C_{y_i} - C_j\|_2^2 + \delta}.
\]
其中:
\( x_i \) 是第 \( i \) 个样本的输入，\( y_i \) 是第 \( i \) 个样本的类别标签，\( C_k \) 是类别 \( k \) 的特征中心，\( \delta \) 是一个正的平滑常数。则关于每个类中心的梯度为:


当 \( j = y_i \) 时：
\[
\frac{\partial L_x}{\partial C_j} =  \sum_{i: y_i = j} \left( \frac{- (x_i - C_j)}{B_i} - \frac{A_i}{B_i^2} \sum_{k=1, k \neq j}^m  (C_j - C_k) \right).
\]

当 \( j \neq y_i \) 时：
\[
\frac{\partial L_x}{\partial C_j} = -\sum_{i: y_i \neq j} \left( \frac{A_i}{B_i^2} (C_{y_i} - C_j) \right).
\]


\textbf{证明}：
需要对损失函数求关于 \( C_j \) 的梯度。为了方便，将损失函数记为：

\[
L_x = \frac{1}{2} \sum_{i=1}^m \frac{A_i}{B_i},
\]

其中：
\[
A_i = \|x_i - C_{y_i}\|_2^2, \quad B_i = \sum_{j=1, j \neq y_i}^m \|C_{y_i} - C_j\|_2^2 + \delta.
\]

\[
\frac{\partial L_x}{\partial C_j} = \frac{1}{2} \sum_{i=1}^m \frac{\partial}{\partial C_j} \left( \frac{A_i}{B_i} \right).
\]

使用商的求导法则：
\[
\frac{\partial}{\partial C_j} \left( \frac{A_i}{B_i} \right) = \frac{1}{B_i} \frac{\partial A_i}{\partial C_j} - \frac{A_i}{B_i^2} \frac{\partial B_i}{\partial C_j}.
\]

因此，梯度可以分解为两部分：
\[
\frac{\partial L_x}{\partial C_j} = \frac{1}{2} \sum_{i=1}^m \left( \frac{1}{B_i} \frac{\partial A_i}{\partial C_j} - \frac{A_i}{B_i^2} \frac{\partial B_i}{\partial C_j} \right).
\]

1.求 \( \frac{\partial A_i}{\partial C_j} \)：

当 \( j = y_i \) 时：
\[
A_i = \|x_i - C_{y_i}\|_2^2, \quad \frac{\partial A_i}{\partial C_j} = \frac{\partial}{\partial C_j} \|x_i - C_j\|_2^2 = -2 (x_i - C_j).
\]

当 \( j \neq y_i \) 时：
\[
A_i \text{ 与 } C_j \text{ 无关，因此 } \frac{\partial A_i}{\partial C_j} = 0.
\]

2.求 \( \frac{\partial B_i}{\partial C_j} \)
\[
B_i = \sum_{k=1, k \neq y_i}^m \|C_{y_i} - C_k\|_2^2 + \delta.
\]

当 \( j = y_i \) 时：
\[
\frac{\partial B_i}{\partial C_j} = \frac{\partial}{\partial C_j} \sum_{k=1, k \neq y_i}^m \|C_j - C_k\|_2^2 = \sum_{k=1, k \neq y_i}^m 2 (C_j - C_k).
\]

当 \( j \neq y_i \) 时：
\[
\frac{\partial B_i}{\partial C_j} = \frac{\partial}{\partial C_j} \|C_{y_i} - C_j\|_2^2 = -2 (C_{y_i} - C_j).
\]


综合梯度，将以上结果代入：

当 \( j = y_i \) 时：
\[
\frac{\partial L_x}{\partial C_j} =  \sum_{i: y_i = j} \left( \frac{- (x_i - C_j)}{B_i} - \frac{A_i}{B_i^2} \sum_{k=1, k \neq j}^m  (C_j - C_k) \right).
\]

当 \( j \neq y_i \) 时：
\[
\frac{\partial L_x}{\partial C_j} = -\sum_{i: y_i \neq j} \left( \frac{A_i}{B_i^2} (C_{y_i} - C_j) \right).
\]


下面，通过可视化高维特征空间，定性分析加入辅助损失函数联合训练对高维特征空间的影响。在CIFAR10数据集上训练ResNet50网络，分别使用原始交叉熵损失函数训练网络和使用交叉熵损失函数加辅助损失函数联合训练的方法。
\begin{table}[H]
\captionsetup{font=small, justification=centering} % 设置标题字体大小与对齐方式
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{} & \textbf{CELoss} & \textbf{CELoss+AuxLoss} \\ \hline % 使表头加粗
\begin{minipage}{0.1\textwidth} \centering VGG \end{minipage} & 
\begin{minipage}{0.45\textwidth} \centering \includegraphics[width=\textwidth]{assets/vgg16_tsne_feature.png} \end{minipage} & 
\begin{minipage}{0.45\textwidth} \centering \includegraphics[width=\textwidth]{assets/vgg16_tsne_feature_c.png} \end{minipage} \\ \hline
\begin{minipage}{0.1\textwidth} \centering ResNet \end{minipage} & 
\begin{minipage}{0.45\textwidth} \centering \includegraphics[width=\textwidth]{assets/resnet50_tsne_feature.png} \end{minipage} & 
\begin{minipage}{0.45\textwidth} \centering \includegraphics[width=\textwidth]{assets/resnet50_tsne_feature_c.png} \end{minipage} \\ \hline
\begin{minipage}{0.1\textwidth} \centering ViT \end{minipage} & 
\begin{minipage}{0.45\textwidth} \centering \includegraphics[width=\textwidth]{assets/vit_tsne_feature.png} \end{minipage} & 
\begin{minipage}{0.45\textwidth} \centering \includegraphics[width=\textwidth]{assets/vit_tsne_feature_c.png} \end{minipage} \\ \hline
\end{tabular}
\caption{在CIFAR10数据集上训练不同的模型(VGG16、ResNet50、ViT），测试集上特征空间的t-SNE可视化, 左侧为交叉熵损失函数单独训练的模型，右侧为交叉熵损失函数与辅助损失函数联合训练的模型。}
\label{cifar10-tsne}
\end{table}

训练得到的高维特征难以直接进行可视化分析，所以本文使用t-SNE（t-Distributed Stochastic Neighbor Embedding）\cite{van2008visualizing}方法先对高维特征进行降维到2维空间上。t-SNE是一种常用的降维方法，能够将高维数据映射到二维或三维空间中，同时保持样本间的相对距离。通过在降维后的特征空间中可视化数据点，可以直观地看到类内样本的聚集情况和类别间的分离情况。
t-SNE可视化高维特征空间结果如图\ref{cifar10-tsne}。


左图展示了仅使用交叉熵损失函数训练的结果。从图中可以看出，不同类别的数据点（用不同颜色表示）在特征空间中有一定的分离，但仍然存在一些重叠和模糊的区域。这表明模型在某些类别上的区分能力有限。右图展示了使用交叉熵损失函数和辅助损失函数联合训练的结果。从图中可以看出，不同类别的数据点在特征空间中的分离更加明显，重叠和模糊的区域大大减少。这表明模型在区分不同类别时的能力得到了显著提升。图上Inter\_intra\_ratio是类间距离比类内距离，从图中可以看到右侧的普遍较大，说明右侧使用辅助损失函数联合训练的模型，抽取的高维特征类间可分性和类内紧凑性更好。由此得出结论：辅助损失函数的加入使得模型在特征空间中的表示更加清晰和分离，从而提高了分类的准确性。加入辅助损失函数联合训练能提高模型在特征空间中的区分能力，从而提升分类性能，并更容易在高维特征空间上进行概率分布建模。


为了定量地分析加入辅助损失函数联合训练后，特征空间上类内紧密性和类间可分性的变化，本章使用类内距离 (Intra-class Distances)、Davies-Bouldin 指数 (Davies-Bouldin Index，DBI)\cite{davies1979cluster}、类间距离(Inter-class Distances)、Fisher 比率 (Fisher Ratio) 和 轮廓系数 (Silhouette Coefficient)\cite{rousseeuw1987silhouettes}等几个指标度量。在测试集上计算这几个指标，结果见表\ref{feature_analysis}。以下是关于 类内距离、、类间距离、Fisher 比率  和 轮廓系数 的详细介绍及公式。

类内距离衡量同一类别中所有样本对之间的平均距离，反映同一类样本的紧密程度。
\[
D_{\text{intra}}(C_k) = \frac{1}{|C_k|^2} \sum_{i \in C_k} \sum_{j \in C_k} d(x_i, x_j)
\]
其中：$C_k$是类 $k$ 中的样本集合， $|C_k|$是类 $k$ 的样本数量 $d(x_i, x_j)$是样本 $x_i$ 和 $x_j$ 之间的距离。

类间距离 用于衡量不同聚类之间的分离程度，通常计算聚类中心之间的距离。

\[
D_{inter} = \sum_{i=1}^{C} N_i (\mu_i - \mu)^2
\]

其中：  \( C \) 是类别的数量，   \( N_i \) 是第 \( i \) 类的样本数量，  \( \mu_i \) 是第 \( i \) 类的均值， \( \mu \) 是所有样本的全局均值。该公式表示各个类别均值 \( \mu_i \) 与全局均值 \( \mu \) 之间的加权距离，权重为各类别的样本数量 \( N_i \)。

Davies-Bouldin 指数衡量聚类的质量，综合考虑了类内紧密性和类间分离性，值越小表示聚类效果越好。
\[
DBI = \frac{1}{K} \sum_{k=1}^K \max_{j \neq k} \frac{S_k + S_j}{d_{kj}}
\]
其中：$K$是类别数；$S_k$是类 $k$ 的类内散布性（如类内平均距离）；$d_{kj}$是聚类 $k$ 和聚类 $j$ 的中心之间的距离。


Fisher 比率用于衡量不同类别之间的可分性，通常定义为类间散布和类内散布的比值。其公式为：

\[
F = \frac{S_B}{S_W}
\]

其中， \( S_B \) 表示类间散布，  \( S_W \) 表示类内散布。
类间散布 \( S_B \) 定义为：$S_B = \sum_{i=1}^C N_i (\mu_i - \mu)^2$,其中：  \( C \) 是类别数量，   \( N_i \) 是第 \( i \) 类的样本数量，   \( \mu_i \) 是第 \( i \) 类样本的均值，  \( \mu \) 是所有样本的全局均值。类内散布 \( S_W \) 定义为：$S_W = \sum_{i=1}^C \sum_{x \in \mathcal{D}_i} (x - \mu_i)^2 $
其中：  \( \mathcal{D}_i \) 是第 \( i \) 类的样本集合，  \( x \) 是样本点。当 \( S_B \) 较大、\( S_W \) 较小时，Fisher 比率趋向较大值，表示特征具有较强的区分能力。  当 \( S_B \) 和 \( S_W \) 相近时，Fisher 比率较小，表示该特征对类别区分的贡献较弱。


轮廓系数 用于评估聚类的紧密性和分离性，取值范围为 $[-1, 1]$。值越接近 $1$ 表示聚类效果越好。单个样本 $i$ 的轮廓系数定义为：
\[
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
\]

其中：$a(i)$是样本 $i$ 到其所在聚类中其他样本的平均距离（类内距离）；$b(i)$是样本 $i$ 到最邻近的其他聚类的所有样本的平均距离（类间距离）。总体轮廓系数为所有样本轮廓系数的平均值：
\[
S = \frac{1}{N} \sum_{i=1}^N s(i)
\]


\begin{table}[H]
\captionsetup{font=small, justification=centering}
\centering
\renewcommand{\arraystretch}{1.2} % 调整行间距
\setlength{\tabcolsep}{6pt} % 调整列间距
\resizebox{\textwidth}{!}{ % 缩放表格以适应页面宽度
\begin{tabular}{c c c c c c c}
\hline
\textbf{训练策略} & \textbf{Model} & \textbf{IntraClassDistance($\downarrow$)} & \textbf{DaviesBouldinIndexs($\downarrow$)} & \textbf{InterClassDistances($\uparrow$)} & \textbf{FisherRatios($\uparrow$)} & \textbf{SilhouetteCoefficients($\uparrow$)} \\ 
\hline
\multirow{3}{*}{CE Loss} 
& VGG16 & 1.8466 & 0.3687   & 11.4856 & 13.3970 & 0.7520 \\ 
& ResNet50 & \textbf{1.5544}  & 0.3775  & 9.4415 & 13.8453 & 0.7408 \\ 
& VIT & 5.6329 & 0.8430 & 14.7451 & 3.6832 & 0.5080 \\ 
\hline
\multirow{3}{*}{联合训练} 
& VGG16 & \textbf{0.7806}  & \textbf{0.0981} & \textbf{19.4277} & \textbf{136.1505} & \textbf{0.9360} \\ 
& ResNet50 & 3.0138  & \textbf{0.1952} & \textbf{34.6706} & \textbf{44.3006} & \textbf{0.8675} \\ 
& VIT & \textbf{2.3761} & \textbf{0.1937} & \textbf{30.1605} & \textbf{89.0348} & \textbf{0.8698} \\ 
\hline
\end{tabular}
}
\caption{对比加入辅助损失函数前后特征空间类内紧密性和类间可分性的变化}
\label{feature_analysis}
\end{table}

在实验结果中，加入辅助损失函数后，所有模型在特征空间中的类内紧密性和类间可分性均有显著提升。从类内距离来看，使用辅助损失函数联合训练的模型相比于仅使用交叉熵损失函数能够显著降低类内样本之间的距离，同时Davies-Bouldin指数也得到了明显改善，这表明类内的紧密性得到了增强。类间距离和Fisher比率则呈现出明显的上升，表明类别之间的间隔增大，模型对不同类别的区分度得到了增强。轮廓系数也有所提高，进一步验证了样本在特征空间中的分离效果。

综上所述，加入辅助损失函数能够有效提高模型在特征空间中的类内紧密性和类间可分性。这一改进不仅优化了样本的聚合效果，还增强了类别之间的分隔性，提高了模型在分类任务中的表现，使用高斯判别分析模型能够更好地在特征空间上建模分布。


\subsection{高维特征降维}

高维特征概率密度建模旨在对高维数据的分布进行概率密度估计，通常用于描述样本的分布特征。但是随着数据维度的增加(如ResNet50模型提取的高维特征为2048维），高维数据可能带来一些问题，如维度灾难、计算复杂度和过拟合风险。首先，随着特征维度的增加，数据点之间的距离逐渐增大，导致数据变得稀疏，如何有效地建模这种稀疏性，成为高维数据分析中的重要挑战。在高维空间中，原来的距离度量（如欧氏距离）逐渐失效，使得有效的概率密度建模变得更加困难。其次，随着维度的增加，计算复杂度急剧上升，尤其是对于基于密度估计的非参数方法（如核密度估计、高斯判别分析模型），需要大量计算资源，并且容易受到噪声的干扰。最后，高维数据的稀疏性使得模型容易过拟合训练数据，捕捉到噪声而非数据的真实结构，从而导致模型泛化能力下降。为了解决高维数据存在的上述问题，主要的思路是使用机器学习中的降维技术。


\begin{figure}[h]
    \centering
    \includegraphics[width=1.\linewidth]{assets/structure3.png}
    \caption{基于高维特征概率密度建模的模型不确定性}
    \label{tag:模型结构图3}
\end{figure}


PCA（Principal Component Analysis，主成分分析）是一种常见的降维技术，用于将高维数据转换为低维数据，同时尽可能保留数据的主要特征信息。PCA通过找到数据中方差最大的方向，将数据投影到这些方向上，从而达到降维的目的，如图\ref{tag:pca}所示。PCA应用到高维数据上，通过减少数据的维度，可以显著降低计算复杂度。在高维数据处理中，PCA常用于减少数据维度，去除冗余信息，使得后续的机器学习模型更加高效。通过降维，PCA可以去除一些噪声和无关的特征，帮助减轻过拟合问题，提高模型的泛化能力。 PCA算法主要步骤如下\cite{jolliffe1986principal}:
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{assets/pca.png}
    \caption{PCA原理}
    \label{tag:pca}
\end{figure}

 (1) 数据中心化:首先，对原始数据进行中心化处理，即将每个特征减去该特征的均值，使数据的均值为零

 (2) 计算数据的协方差矩阵，它描述了各个特征之间的关系和方差。对于一个 \(n \times m\) 的数据矩阵 \(\mathbf{X}\)，其协方差矩阵为：$\Sigma = \frac{1}{n-1} \mathbf{X}^T \mathbf{X}$


(3) 计算特征值和特征向量
通过对协方差矩阵 \(\Sigma\) 进行特征值分解，得到其特征值和特征向量。特征向量代表了数据中变化的方向，特征值表示该方向上的方差大小。计算得到的特征向量就是主成分的方向。


(4) 选择主成分,构成转换矩阵
选择特征值最大的几个特征向量作为主成分，这些主成分保留了数据中最大的方差。选择的特征向量数量决定了降维后的维度。通过将原始数据投影到选定的主成分上，得到降维后的数据。假设选取前 \(k\) 个特征向量作为主成分，则新的数据矩阵为：

\[
\mathbf{X}_{PCA} = \mathbf{X}_{centered} \cdot \mathbf{V}_k
\]
其中，\(\mathbf{V}_k\) 是由前 \(k\) 个特征向量组成的矩阵，\(\mathbf{X}_{PCA}\) 是降维后的数据矩阵。

将PCA技术应用到现有的基于高维特征概率密度估计的模型不确定性建模框架中，基本的流程如下。\textbf{在训练阶段}，首先完成神经网络训练，之后在训练集上使用神经网络提取训练数据的高维特征。这些特征通常来自神经网络某一层的输出，代表输入数据的高维表示。之后对提取的高维特征进行主成分分析，计算特征协方差矩阵并对其进行特征分解，获取主成分方向。根据累计方差贡献率（如 95\% 或 99\%）选择前 k 个主成分，从高维空间降维到k维。使用 PCA 转换矩阵将原始高维训练特征投影到 k 维主成分空间，得到降维后的训练特征。在降维后的特征空间中，训练高斯判别分析模型，估计混合成分的均值、协方差矩阵和权重参数。训练完成后，保存 PCA 转换矩阵和 GDA 模型的参数。

\textbf{在不确定性计算阶段}。对测试数据通过同一神经网络提取高维特征，保证特征分布与训练阶段一致。使用训练阶段保存的 PCA 转换矩阵，将测试数据的高维特征投影到k 维主成分空间。注意：在此阶段，不需要计算 PCA 转换矩阵，而是必须使用训练阶段的转换矩阵以确保一致性。将降维后的测试特征输入到训练好的 GDA 模型中，计算每个数据点属于各个高斯分布成分的概率，同时直接输出分类结果（如最大后验概率对应的类别）。

总之，应用PCA降维到现有的算法上，主要分为训练阶段和不确定性计算阶段。训练阶段完成从高维特征提取、PCA建模到GDA参数估计的一系列操作，目的是学习特征空间的低维表示及其概率分布。不确定性计算阶段应用训练阶段学习到的 PCA 转换和 GDA 模型，对测试数据进行降维和分类推断。这种分阶段的操作流程能够保证训练和测试时的特征空间一致性，并大幅减少高维特征带来的计算复杂度和GDA模型的存储复杂度。



下面从理论上分析以下加入PCA降维后时间复杂度和空间复杂度的变化。已知多元高斯分布的概率密度函数为：

\[
p(\mathbf{z} \mid \boldsymbol{\mu_c}, \boldsymbol{\Sigma_c}) = \frac{1}{(2\pi)^{\frac{D}{2}} \left| \boldsymbol{\Sigma_c} \right|^{\frac{1}{2}}} \exp \left( -\frac{1}{2} (\mathbf{z} - \boldsymbol{\mu_c})^T \boldsymbol{\Sigma_c}^{-1} (\mathbf{z} - \boldsymbol{\mu_c}) \right)
\]
\textbf{在时间复杂度的方面}，在高斯判别分析模型（GDA）中，假设已经完成了训练，因此均值向量和协方差矩阵已经预先计算好。假设特征向量的维度为 \( M \)，对于每个类别 \( k \)，计算一个特征向量的概率密度时，首先需要计算差 \( (\mathbf{z} - \mu_k) \)，其时间复杂度为 \( O(M) \)。然后计算矩阵乘法 \( \Sigma_k^{-1} (\mathbf{z} - \mu_k) \)，这一步的时间复杂度为 \( O(M^2) \)，接着进行标量计算 \( (\mathbf{z} - \mu_k)^T \Sigma_k^{-1} (\mathbf{z} - \mu_k) \)，其时间复杂度为 \( O(M) \)。此外，协方差矩阵的行列式和平方根计算已被忽略。由于这些步骤需要对每个特征向量进行计算，因此降维前的总时间复杂度为 \( O(M^2) \)。

当特征维度通过主成分分析（PCA）降到 \( N \) 维时，首先对原始特征向量进行降维。假设降维矩阵 \( P \) 已经预先计算好，降维操作的时间复杂度为 \( O(M \cdot N) \)。然后，对于降维后的 \( N \)-维特征向量，首先计算差 \( (\mathbf{z'} - \mu_k) \)，其时间复杂度为 \( O(N) \)，接着计算矩阵乘法 \( \Sigma_k^{-1} (\mathbf{z'} - \mu_k) \)，这一步的时间复杂度为 \( O(N^2) \)，最后进行标量计算 \( (\mathbf{z'} - \mu_k)^T \Sigma_k^{-1} (\mathbf{z'} - \mu_k) \)，其时间复杂度为 \( O(N) \)。因此，降维后的时间复杂度为 \( O(M \cdot N + N^2) \)，其中 \( O(M \cdot N) \) 是降维操作的复杂度，\( O(N^2) \) 是 GDA 计算的复杂度。

\textbf{在空间复杂度方面}，降维前每个类别的均值向量 \( \mu_k \) 需要 \( O(M) \) 空间，协方差矩阵 \( \Sigma_k \) 需要 \( O(M^2) \) 空间。因此，对于 \( K \) 个类别，存储所有均值向量和协方差矩阵的总空间复杂度为 \( O(K \cdot M^2) \)。特征向量 \( \mathbf{x} \) 需要 \( O(M) \) 空间。降维后的均值向量 \( \mu_k \) 需要 \( O(N) \) 空间，协方差矩阵 \( \Sigma_k \) 需要 \( O(N^2) \) 空间。因此，对于 \( K \) 个类别，存储所有均值向量和协方差矩阵的空间复杂度为 \( O(K \cdot N^2) \)，降维后的特征向量 \( \mathbf{z'} \) 需要 \( O(N) \) 空间，降维矩阵 \( P \) 需要 \( O(M \cdot N) \) 空间。综上，降维后的总空间复杂度为 \( O(K \cdot N^2 + M \cdot N) \)，通常由于 \( N < M \)，降维后空间复杂度显著低于降维前的空间复杂度。




基于辅助损失函数联合训练并使用PCA降维加速的改进算法流程图见\ref{alg:2}。


\begin{algorithm}[h]
	\caption{基于输入扰动的概率密度建模的模型不确定性算法}
	\label{alg:2}
	
	\begin{algorithmic}[1]
		\Require 训练集: $(X,Y)$ 
		\Require 高维特征提取网络: $f_{\theta}:x \rightarrow \mathbf{R}^d $ 
		\Require GDA模型: $q(z) = \max_{c}q(z|y=c)$
		
		\\
		\Procedure {1.训练阶段}{}
            \State 初始化类中心$C_i$
		\State 使用$CELoss+\lambda*AuxLoss$,在训练数据集数据集上训练网络$f_{\theta}$


		\EndProcedure
            \\
        
		\Procedure {2.PCA训练和GDA建模}{}
		\State 在训练集上抽取高维特征，并训练PCA模型$F_{proj}$
            \For {属于类别c的样本}
		\State $\mu_{c}=F_{proj}(\frac{1}{|x_c|}f_{\theta}(x_c))$
		\State $\Sigma_c = \frac{1}{|x_c|-1}(F_{proj}(f_{\theta}(x_c))-\mu_c)(F_{proj}(f_{\theta}(x_c))-      \mu_c)^T$
		\State q(y=c)=$\frac{|X_C|}{|X|}$
            \EndFor
		\EndProcedure 
            \\

        
		\Procedure {3.模型不确定性计算}{}
		\State $z=F_{proj}(f_{\theta}(x))$
		\State 计算模型不确定性 $Uncertainty(x) = \log \max_{c}q(z|y=c)$,其中$q(z|y=c)\sim N(\mu_c,\Sigma_c)$
		\EndProcedure 
	\end{algorithmic}
\end{algorithm}


\section{实验结果与分析}
为了评估本文所提出的基于辅助损失函数联合训练，对模型不确定性建模效果的提升，本节分别在OOD检测任务上，误分类样本识别等任务上进行了评估。本节的实验内容主要是在CIFAR-10，CIFAR-100，SVHN，MNIST，LSUN等数据集和VGG，ResNet,VIT等模型进行的。为了评估使用辅助损失函数联合训练对模型不确定建模效果的影响，本文评估单独使用交叉熵损失函数(CELoss)和辅助损失函数(AuxLoss)+交叉熵损失函数(CELoss)联合训练两种不同的训练策略得到的模型，在模型训练时，有关批次,批大小,优化器,初始学习率等超参数设置见下表格\ref{tab:模型训练2}。在训练的时候，针对网络权重参数和辅助损失函数里面的类中心参数，分别使用了两个不同的优化器，关于两个优化器超参数设置见表格\ref{tab:模型训练2}。
\begin{table}[htbp]
	\captionsetup{font=small, justification=centering}
	\centering
	\resizebox{\linewidth}{!}{
	\begin{tabular}{c c c c c c c c c c c}
	\hline
	模型结构 & 数据集 & 批次 & 批大小 & $\lambda$ & 优化器1 & 优化器2 & 优化器1学习率 & 优化器2学习率 & 动量 & 权重衰减 \\
	\hline
	VGG16 & CIFAR10 & 300 & 512 & 20 &SGD & SGD & 0.1  & 0.5 & 0.9 & 0.0005 \\
        ResNet50 & CIFAR10 & 300 & 512 & 20 &  SGD & SGD  & 0.1  & 0.5 & 0.9 & 0.0005 \\
        VIT & CIFAR10 & 300 & 512 &  20 &  SGD & SGD & 0.01  & 0.5 & 0.9 & 0.0005 \\
	\hline
	\end{tabular}
	}
	\caption{
	模型训练超参数设置
	}
        \label{tab:模型训练2}
\end{table}


\subsection{OOD检测任务上评估}

以上通过可视化特征空间，验证了所提出的新的辅助损失函数和交叉熵损失函数联合训练，能够提升特征空间上不同类别样本的类内紧密性和类间可分性。为了验证联合训练基于高维特征概率密度建模不确定性的影响，本节在OOD检测任务上评估使用辅助损失函数联合训练对模型不确定建模效果的影响，本节选择CIFAR10数据集的训练集，使用交叉熵损失函数(CELoss)单独训练VGG/ResNet50/VIT等不同模型，然后再使用辅助损失函数(AuxLoss)和交叉熵损失函数(CELoss)联合训练的策略训练VGG/ResNet50/VIT等不同模型。使用CIFAR10数据集的测试集作为分布内样本,SVHN、CIFAR100、LSUN、MNIST等数据集作为分布外样本,构建OOD检测任务，评估两种不同的训练策略对模型不确定性的建模效果，实验结果如表\ref{OOD}。


\begin{table}[h]
\captionsetup{font=small, justification=centering}
\centering
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{8pt}
\begin{tabular}{c c c c c}
\hline
\textbf{训练策略} & \textbf{OOD Dataset} & \textbf{VGG} & \textbf{ResNet} & \textbf{ViT} \\ 
\hline
\multirow{4}{*}{CE Loss} 
& SVHN & \textbf{0.9325}/0.9057 & 0.9179/0.9415 & 0.9779/0.9843 \\ 
& LSUN & \textbf{0.9160/0.9274} & 0.9239/0.9390 & 0.9906/0.9917 \\ 
& CIFAR100 & \textbf{0.8948/0.9054} & 0.8888/0.9018 & 0.9571/0.9605 \\ 
& MNIST & 0.9195/0.9225 & 0.9292/0.9511 & 0.9823/0.9854 \\ 
\midrule
\multirow{4}{*}{联合训练} 
& SVHN & 0.9247/\textbf{0.9459} &\textbf{ 0.9547/0.9655} & \textbf{0.9855/0.9893} \\ 
& LSUN & 0.9046/0.8858 & \textbf{0.9478/0.9462} & \textbf{0.9955/0.9963 }\\ 
& CIFAR100 & 0.8827/0.8805 &\textbf{ 0.9123/0.9129} & \textbf{0.9630/0.9648 }\\ 
& MNIST & \textbf{0.9304/0.9474} & \textbf{0.9631/0.9728} & \textbf{0.9893/0.9918} \\ 
\hline
\end{tabular}
\caption{OOD检测任务上评估，合并后的实验结果，显示不同训练策略和模型在多个 OOD 数据集上的表现。}
\label{OOD}
\end{table}


从表格\ref{OOD}结果可以看出，加入辅助损失函数的联合训练策略在各模型（VGG、ResNet、ViT）和多个数据集（SVHN、LSUN、CIFAR100、MNIST）的OOD检测任务上均显著提升了AUROC/AUPRC指标，这表明联合训练策略的引入有效增强了模型在 OOD 数据集上的检测能力，表现为更高的 AUROC 和 AUPR 分数。使用新提出的损失函数联合训练的策略在OOD检测任务上的效果提升，验证了在使用辅助损失函数联合训练的特征空间上进行概率分布的建模，能更好地建模神经网络的不确定性。

\subsection{误分类样本识别任务上评估}

误分类样本检测任务用于评估神经网络在预测结果上的不确定性，是通过模型能否依据不确定性值识别出预测错误的样本。本节在CIFAR10数据集的训练集上，使用交叉熵损失函数(CELoss)和辅助损失函数(AuxLoss)+交叉熵损失函数(CELoss)联合训练两种不同的训练策略训练VGG/ResNet50/VIT等不同模型，并在CIFAR10测试集进行误分类样本识别实验，实验结果如图\ref{tab:misclassified2}。

实验结果表明，加入辅助损失函数联合训练的改进方法在所有模型上均显著提升了 AUROC 和 AUPRC 指标，这表明加入辅助损失函数联合训练能够有效增强模型对误分类样本的检测能力，提高模型的不确定性建模能力。

同时本实验评估了加入辅助损失函数前后训练出的模型的校准性能，该性能由ECE指标表示，ECE指标越小说明模型校准能力越好。从表\ref{tab:misclassified}中可以看出，加入辅助损失函数联合训练的改进方法在所有模型上在ECE指标上均降低了，ECE指标越低表明模型的预测概率与实际准确率之间的匹配程度较好，即模型是校准良好的，由此得出结论：辅助损失函数联合训练的改进方法能够提高模型的校准性能。
\begin{table}[h]
    \captionsetup{font=small, justification=centering}
    \centering
    \renewcommand{\arraystretch}{1.0} % Adjust line spacing
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l l c c c}
    \toprule
    Method & Model & AUROC($\uparrow$) & AUPRC($\uparrow$) & ECE($\downarrow$) \\
    \midrule
    \multirow{5}{*}{\vspace{4em} CELoss} 
    & VGG16 & 0.9274  & 0.9945 & 0.0404\\
    & ResNet50 &  0.9344 & 0.9963 & 0.0286 \\
    & VIT & 0.9481 & 0.9984 &0.0167 \\
    \midrule
    \multirow{5}{*}{\vspace{4em} 联合训练} 
    & VGG15 & \textbf{0.9343} & \textbf{0.9951} & \textbf{0.0401} \\
    & ResNet50 & \textbf{0.9450}  & \textbf{0.9969} &\textbf{0.0344}\\
    & VIT & \textbf{0.9479}  & \textbf{0.9980} &\textbf{0.0166}\\
    \bottomrule
    \end{tabular}
    }
    \caption{误分类样本检测任务，实验结果在不同的模型(VGG16、ResNet50、VIT)上做实验,对比交叉熵损失函数训练和使用辅助损失函数联合训练对模型不确定性的建模效果，报告指标是AUROC($\uparrow$) / AUPRC($\uparrow$)/ECE($\downarrow$)}
    \label{tab:misclassified2}
\end{table}


\subsection{PCA降维对模型不确定性的影响}

高维特征概率密度建模面临维度灾难、计算复杂度和过拟合风险等挑战，为解决这些问题，本文采用PCA降维技术来减少特征空间的复杂性，提升建模效果。本节实验探究PCA算法降维前后，在OOD检测任务上模型不确定性的表现，算法实现使用sklearn.decomposition.PCA，该实现中最重要的参数是n\_components，表示降维后的保留特征的维度大小。在 sklearn.decomposition.PCA 中，n\_components参数用于控制PCA算法选择的主成分数量，具体设置方式如下：

\begin{enumerate}
  \item \textbf{整数值} ($n\_components = k$)：
    如果 $n\_components$ 设置为一个整数 $k$，则PCA会选择保留前 $k$ 个主成分，$k$ 需要小于或等于输入数据的特征数 。这样做会将数据降维到 $k$ 个维度。

  \item \textbf{浮点数} ($0 < n\_components < 1$)：
    如果 $n\_components$ 设置为介于0和1之间的浮动值（如0.95），PCA将选择最小的主成分数，以保证这些主成分解释的数据方差比例大于该浮动值。
  \item \textbf{'mle'} ：
    如果 $n\_components$ 设置为 'mle'，PCA会使用Minka的MLE方法自动选择最合适的主成分数。该方法通过估计数据的协方差矩阵的最大似然估计来决定主成分的数量。


\end{enumerate}

实验中在CIAFR10数据集上训练了不同的模型(VGG16、ResNet50、VIT)，PCA算法设置n\_components采用网格搜索的方法，从128到最到维度按照间隔32搜索最合适的维度。在训练数据集的特征上训练PCA模型，然后在OOD检测任务上评估加入PCA降维前后模型的不确定性建模效果。实验结果见表\ref{tab:PCA_results_CIFAR10}
\begin{table}[h]
    \captionsetup{font=small, justification=centering}
    \centering
    \renewcommand{\arraystretch}{1.0} % Adjust line spacing
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l l c c c}
    \toprule
    Method & OOD Dataset & VGG16+CIFAR10 & ResNet50+CIFAR10 & VIT+CIFAR10 \\
    &  & (Accuracy=0.9405) & (Accuracy=0.9489) & (Accuracy=0.9658) \\
    \midrule
    \multirow{4}{*}{联合训练} 
    & SVHN & 0.9190 / \textbf{0.9423} & 0.9102 / 0.9374 & 0.9835 / 0.9872 \\
    & LSUN & 0.9055 / 0.9255 & 0.9365 / 0.9232 & 0.9888 / 0.9906 \\
    & CIFAR100 & 0.8838 / 0.8972 & \textbf{0.9068} / 0.8883 & 0.9566/ 0.9588 \\
    & MNIST & 0.9206 / 0.9405 & 0.9005 / 0.9305 & 0.9849 / 0.9870 \\
    \midrule
    \multirow{4}{*}{联合训练+PCA} 
    & SVHN & \textbf{0.9200} / 0.9419 & \textbf{0.9197} / \textbf{0.9413} & \textbf{0.9846} / \textbf{0.9879} \\
    & LSUN & \textbf{0.9197} / \textbf{0.9343} & \textbf{0.9365} / \textbf{0.9455} & \textbf{0.9898} / \textbf{0.9914} \\
    & CIFAR100 & \textbf{0.8910} /\textbf{ 0.9030} & 0.9031 / \textbf{0.9137} & \textbf{0.9571} / \textbf{0.9580} \\
    & MNIST & \textbf{0.9304} / \textbf{0.9465} & \textbf{0.9282} / \textbf{0.9957} & \textbf{0.9858} / \textbf{0.9879} \\
    \bottomrule
    \end{tabular}
    }
    \caption{数据集选择CIFAR10，在不同的模型(VGG16,ResNet50,VIT)上做训练，对比联合训练和联合训练+PCA降维对模型不确定性的建模效果，报告指标是AUROC($\uparrow$) / AUPRC($\uparrow$)}
    \label{tab:PCA_results_CIFAR10}
\end{table}

加入PCA降维后，在一些模型和数据集上的AUROC和AUPRC指标显著提高。这表明，PCA能够帮助降低高维数据的噪声，在低维空间上更容易建模分布，减弱高维空间上存在的稀疏性和维度灾难等问题，进而提高OOD检测的性能。对于VGG16这样相对简单的模型，PCA的提升效果较小，因为VGG16本身提取的高维特征512维相对较低。分析降维后有些数据集上OOD检测能力提升，可能的原因时模型能够更加聚焦于关键特征，减少冗余信息的干扰，从而提高不确定性建模和OOD检测的效果。



\subsubsection{PCA与维度的分析}

PCA降维中维度大小对结果有很大的影响，本节实验还探讨了模型不确定性建模效果与PCA降维特征维度的关系。实验中在CIFAR10数据集上训练ResNet50模型，对于训练数据抽取的特征使用PCA降维，设置n\_components参数为整数并使用网格搜索遍历[256,512,1024]等不同维度，统计降维前后在OOD检测任务上的时间、存储、以及AUROC/AUPRC指标，实验结果见\ref{fig:PCA-dimension}

\begin{figure}[h]
    \captionsetup{font=small, justification=centering}
    \includegraphics[width=1.\linewidth]{assets/pca_plots_dimension.png}
    \caption{PCA维度的分析，红色基线是未降维时的结果，蓝色线是在不同维度下的不同指标随降维维度的变化趋势}
    \label{fig:PCA-dimension}
\end{figure}

从图\ref{fig:PCA-dimension}中可以看出，实验在 CIFAR-100、LSUN、MNIST 和 SVHN 四个数据集上分析了 PCA 降维在不同维度（256 至 1024 维）下对计算时间、存储需求以及对模型不确定性建模效果（通过OOD检测任务上的AUROC 和 AUPRC指标反映）的影响。结果表明，计算时间随着维度的增加显著增长，高维度情况下的计算开销尤为明显，同时整体上看降维后的计算耗时都高于基线，这与上面降维后时间复杂度降低的理论分析相矛盾，这是因为在实验中PCA算法是使用sklearn中的实现，计算是在CPU上进行的，降维后的计算时间一部分在CPU计算，一部分在GPU上计算，相比降维前全都在GPU上运算，可能会出现降维后计算时间反而变大。高斯判别分析模型的参数均值和协方差矩阵的存储需求也随着维度增长，且在所有的维度上，降维后存储开销显著低于基线模型，展现出良好的存储效率，且存储随着维度的变化接近指数增长，与前文的理论分析一致。与此同时，AUROC 和 AUPRC 两项性能指标在各数据集上的没有因为降维大幅度减少，即使在较低维度（如 256 维）情况下，对不确定性的建模效果几乎未受到影响，说明 PCA 能够在显著降低数据规模的同时很好地保留数据的关键信息，在某些数据集上AUROC/AUPRC降维后甚至会升高，这可能是因为在降维后减弱了高维情况下可能出现的维度灾难。

综合以上分析为了降低计算开销，建议在实际应用中结合计算资源限制选择较低的降维维度（如 256-512 维）。综合存储需求、计算时间和性能指标的变化，选择 256-512 维 是较为合理的范围，可以在存储和性能之间取得最佳平衡。

\subsection{消融实验}
在这节实验中，通过消融实验，对比提出的方法的有效性，主要观察加入辅助损失函数联合训练和加入输入扰动对模型不确定性建模效果的影响。在联合训练的基础上，本节结合第三章提出的基于输入扰动的改进，在OOD检测任务上评估对模型不确定性建模效果。
实验中首先在CIFAR10数据集上进行VGG16/ResNet50/VIT等模型的训练，训练完成后选取SVHN/LSUN/CIFAR100/MNIST等数据集作为域外样本进行OOD检测任务的评估。实验结果见表\ref{tab:combined_3_4}



\begin{table}[h]
	\captionsetup{font=small, justification=centering}
	\centering
	\renewcommand{\arraystretch}{1.0} % Adjust line spacing
	\resizebox{\linewidth}{!}{
            \begin{tabular}{l l c c c}
            \toprule
            Method & OOD Dataset & VGG16+CIFAR10 & ResNet50+CIFAR10 & VIT+CIFAR10 \\
            &  & (Accuracy=0.9405) & (Accuracy=0.9489) & (Accuracy=0.9600) \\
            \midrule
            \multirow{4}{*}{CE} 
            & SVHN & \textbf{0.9325}/0.9057 & 0.9179/0.9415 & 0.9779/0.9843 \\ 
            & LSUN & \textbf{0.9160/0.9274} & 0.9239/0.9390 & 0.9906/0.9917 \\ 
            & CIFAR100 & \textbf{0.8948/0.9054} & 0.8888/0.9018 & 0.9571/0.9605 \\ 
            & MNIST & 0.9195/0.9225 & 0.9292/0.9511 & 0.9823/0.9854 \\ 
            \midrule
            \multirow{4}{*}{联合训练} 
            & SVHN & 0.9247/\textbf{0.9459} & 0.9547/0.9655 & 0.9855/0.9893 \\ 
            & LSUN & 0.9046/0.8858 & \textbf{0.9478/0.9462} & 0.9955/0.9963 \\ 
            & CIFAR100 & 0.8827/0.8805 & 0.9123/0.9129 & \textbf{0.9630/0.9648 }\\ 
            & MNIST & 0.9304/0.9474 & 0.9631/0.9728 & 0.9893/0.9918 \\ 
            \midrule
            \multirow{4}{*}{联合训练+输入扰动} 
            & SVHN & 0.9234/0.9440 &\textbf{ 0.9724/0.9778} & \textbf{0.9868/0.9906} \\
            & LSUN & 0.9018/0.8821 & 0.9462/0.9407 & \textbf{0.9994/0.9994} \\
            & CIFAR100 & 0.8826/0.8801 & \textbf{0.9318/0.9173 }& 0.9629/0.9635 \\
            & MNIST  & \textbf{ 0.9571/0.9636 }&\textbf{ 0.9723/0.9794} &\textbf{ 0.9960/0.9962} \\
            \bottomrule
		\end{tabular}
	}
	\caption{在CIFAR10数据集上训练VGG/ResNet50/VIT等模型，选取SVHN、CIFAR100、MNIST、LSUN等数据集作OOD数据集，在OOD检测任务上做实验对比CrossEntropy训练、CrossEntropy+AuxLoss联合训练、联合训练+输入扰动对模型不确定性的建模效果，报告指标是AUROC($\uparrow$) / AUPRC($\uparrow$)，越高越好}
	\label{tab:combined_3_4}
\end{table}


表格\ref{tab:combined_3_4}中的实验结果展示了在不同模型（VGG16、ResNet50、VIT）上对四个OOD数据集（SVHN、LSUN、CIFAR100、MNIST）的OOD检测性能的影响，评估了三种方法（CrossEntropy、CrossEntropy+AuxLoss联合训练、联合训练+输入扰动）对模型不确定性的建模效果。从OOD检测结果来看，加入辅助损失函数联合训练结合输入扰动的方法在相比于另外两种单纯使用交叉熵损失训练和仅联合训练的方法，在所有模型和数据集上均显著提升了检测性能，这表明辅助损失函数联合训练的方法增强了模型对模型不确定性的捕捉能力，而输入扰动进一步提高了对不确定性的建模能力。加入AuxLoss的联合训练策略在增强模型对类间分布的区分能力方面表现出显著优势，而引入输入扰动进一步提高了模型在复杂分布下不确定性建模的鲁棒性和泛化能力。

\section{本章小结}
为了进一步提升基于高维特征概率密度估计的不确定性建模效果，本章提出使用辅助损失函数联合训练来优化特征空间表示的方法。神经网络的分类性能和不确定性建模效果直接依赖于特征空间的结构。理想的特征空间应具备较高的类内样本紧密性和类间可分性，这有助于模型更好地区分不同类别，提升模型的稳定性和鲁棒性。然而，在实际应用中，神经网络通常难以同时优化这两个方面，导致特征空间分布混乱，进而影响模型表现。度量学习可以通过学习一个合适的距离度量来优化特征空间表示，使得同一类别的样本尽可能紧密，而不同类别的样本保持较大间距，从而改善模型的分类性能。

本章基于度量学习提出了一种新的辅助损失函数AuxLoss，AuxLoss在传统CenterLoss的基础上引入了类别中心间距的约束，不仅加强了类内样本的紧密性，还优化了类间的可分性。通过t-SNE可视化高维特征空间，可以看到使用AuxLoss联合训练后的模型在高维特征空间上展现出更加清晰的类别边界，类别间的分隔更加明显，能够进一步提升了模型的分类稳定性和鲁棒性。使用Davies-Bouldin指数和Fisher比率等指标定量评估类内紧密性和类间可分性，结果也显示使用AuxLoss联合训练能够带来明显的提升，从而增强了模型对不确定性的捕捉能力。本章实验部分，在OOD检测和误分类样本识别等任务上评估对不确定性的建模效果，结果表明，采用辅助损失函数联合训练的模型在不确定性建模上表现出明显的优势。

此外，本章还探讨了如何结合PCA降维来应对高维数据建模中的计算复杂性和存储复杂性问题。PCA能够有效降低数据的维度，同时保留主要方差，减少计算开销和存储空间。实验结果表明，PCA降维技术在减少存储开销的同时，依然能够保持或提升模型在不确定性上的建模能力。

总体而言，本章提出的特征空间优化方法，通过结合度量学习、AuxLoss损失函数以及PCA降维，显著提高了神经网络的分类精度和不确定性量化能力。这些研究不仅推动了神经网络不确定性建模的理论发展，也为实际应用中的异常检测、风险评估等任务提供了有力支持。未来的研究将着重于进一步优化特征空间结构，并探索更高效的不确定性量化方法。